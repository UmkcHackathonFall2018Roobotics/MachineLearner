# Import libs
import tensorflow as tf
import numpy as np
import pandas as pd
import math
from sklearn.preprocessing import MinMaxScaler
from sklearn import metrics
import matplotlib.pyplot as plt

# Import data
data = pd.read_csv('sentiment_priceaction_combined_binary.csv')
#data = pd.read_csv('data_stocks.csv')
# Drop date variable
#data = data.drop(['DATE'], 1)
data = data.drop(['Date'], 1)
data = data.drop(['Title'], 1)
#print(data)

# Dimensions of dataset
n = data.shape[0] #number of rows
p = data.shape[1] #number of cols
# 49718   41

#data table
#data.shape
#data.head(3)

# Make data a np.array
data = data.values #put data into numpy format

# Training and test data
train_start = 0
train_end = int(np.floor(0.8*n))
test_start = train_end + 1
test_end = n
data_train = data[np.arange(train_start, train_end), :] 
data_test = data[np.arange(test_start, test_end), :]


# Scale data
scaler = MinMaxScaler(feature_range=(-1, 1))
scaler.fit(data_train)
scaler.fit(data_test) ##adjsted
data_train = scaler.transform(data_train)
data_test = scaler.transform(data_test)
#print('data_train', data_train)
#print('data_test', data_test)

#Divide training and testing

# Build X and y train
X_train = data_train[:, 1:] #inputs
#print('X_train', X_train)
y_train = data_train[:, 0].reshape((-1,1)) #output
#print('y_train', y_train)

# Build X and y test
X_test = data_test[:, 1:]
#print('X_test', X_test)
y_test = data_test[:, 0].reshape((-1,1))
#print('y_test', y_test)

#print('X_train',X_train)
# Number of stocks in training data
n_stocks = X_train.shape[1]
#print('n_stocks', n_stocks)

# Neurons  
n_neurons_1 = 1024
n_neurons_2 = 512
n_neurons_3 = 256
n_neurons_4 = 200

# removed from network
# n_neurons_5 = 150
# n_neurons_6 = 100
# n_neurons_7 = 50
#

# Session
net = tf.InteractiveSession()

# Placeholder
X = tf.placeholder(dtype=tf.float32, shape=[None, n_stocks]) #data
#print('X', X)
Y = tf.placeholder(dtype=tf.float32, shape=[None, 1]) #target
#print('Y', Y)

# Initializers
sigma = 1
weight_initializer = tf.variance_scaling_initializer(mode="fan_avg", distribution="uniform", scale=sigma)
bias_initializer = tf.zeros_initializer()

# Hidden weights
W_hidden_1 = tf.Variable(weight_initializer([n_stocks, n_neurons_1]))
bias_hidden_1 = tf.Variable(bias_initializer([n_neurons_1]))
W_hidden_2 = tf.Variable(weight_initializer([n_neurons_1, n_neurons_2]))
bias_hidden_2 = tf.Variable(bias_initializer([n_neurons_2]))
W_hidden_3 = tf.Variable(weight_initializer([n_neurons_2, n_neurons_3]))
bias_hidden_3 = tf.Variable(bias_initializer([n_neurons_3]))
W_hidden_4 = tf.Variable(weight_initializer([n_neurons_3, n_neurons_4]))
bias_hidden_4 = tf.Variable(bias_initializer([n_neurons_4]))

# removed from network
# W_hidden_5 = tf.Variable(weight_initializer([n_neurons_4, n_neurons_5]))
# bias_hidden_5 = tf.Variable(bias_initializer([n_neurons_5]))
# W_hidden_6 = tf.Variable(weight_initializer([n_neurons_5, n_neurons_6]))
# bias_hidden_6 = tf.Variable(bias_initializer([n_neurons_6]))
# W_hidden_7 = tf.Variable(weight_initializer([n_neurons_6, n_neurons_7]))
# bias_hidden_7 = tf.Variable(bias_initializer([n_neurons_7]))
#

# Output weights
W_out = tf.Variable(weight_initializer([n_neurons_4, 1]))
bias_out = tf.Variable(bias_initializer([1]))

# Hidden layer
hidden_1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))
hidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))
hidden_3 = tf.nn.relu(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3))
hidden_4 = tf.nn.relu(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4))

# removed from network
# hidden_5 = tf.nn.relu(tf.add(tf.matmul(hidden_4, W_hidden_5), bias_hidden_5))
# hidden_6 = tf.nn.relu(tf.add(tf.matmul(hidden_5, W_hidden_6), bias_hidden_6))
# hidden_7 = tf.nn.relu(tf.add(tf.matmul(hidden_6, W_hidden_7), bias_hidden_7))
#
# Output layer (transpose!)
out = tf.transpose(tf.add(tf.matmul(hidden_4, W_out), bias_out))
#print('out', out)

# Our vectorized labels
#y_train = np.asarray(train_labels).astype('float32').reshape((-1,1))
#y_test = np.asarray(test_labels).astype('float32').reshape((-1,1))

# Cost function
mse = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=out, labels=Y))
#mse = tf.reduce_mean(tf.squared_difference(out, Y))
#print('mse', mse)

# Optimizer
opt = tf.train.AdamOptimizer().minimize(mse)
#print('opt', opt)

# Init
net.run(tf.global_variables_initializer())

# Setup plot
plt.ion()
fig = plt.figure()
ax1 = fig.add_subplot(111)
line1, = ax1.plot(y_test)
line2, = ax1.plot(y_test * 0.5)
plt.show()

# Fit neural net
mse_train = []
mse_test = []

#Loss storage
# Define the variable that stores the result
loss_trace = []
train_acc = []
test_acc = []

from keras.models import Sequential
from keras.layers import Dense

def build_model():
    model = Sequential()
    model.add(Dense(40, input_dim=40, activation='relu'))
    model.add(Dense(40, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model
print('Built Model')
# Run
epochs = 10
batch_size = 256
for e in range(epochs):

    # Shuffle training data
    shuffle_indices = np.random.permutation(np.arange(len(y_train)))
    X_train = X_train[shuffle_indices]
    y_train = y_train[shuffle_indices]
    #print('Shuffled Training Data')
    # Minibatch training
    for i in range(0, len(y_train) // batch_size):
        start = i * batch_size
        batch_x = X_train[start:start + batch_size]
        #print('Batch x', batch_x)
        batch_y = y_train[start:start + batch_size]
        #print('Batch y', batch_y)
        # Run optimizer with batch
        net.run(opt, feed_dict={X: batch_x, Y: batch_y})
        #print('Minibatch Training Complete')
        
        # Show progress => Not working now...
        #if np.mod(i, 50) == 0:
        ## convert into a matrix, and the shape of the placeholder to correspond
        #temp_train_acc = net.run(mse, feed_dict={X: X_train, Y: y_train})
        #temp_test_acc = net.run(mse, feed_dict={X: X_test, Y: y_test})
        # recode the result
        #loss_trace.append(temp_loss)
        #train_acc.append(temp_train_acc)
        #test_acc.append(temp_test_acc)
        # output
        #if (epoch + 1) % 300 == 0:
        #    print('epoch: {:4d} loss: {:5f} train_acc: {:5f} test_acc: {:5f}'.format(epoch + 1, temp_loss,
        ## 
         #                                                                                temp_train_acc, temp_test_acc))
            # MSE train and test
            #mse_train.append(net.run(mse, feed_dict={X: X_train, Y: y_train}))
            #mse_test.append(net.run(mse, feed_dict={X: X_test, Y: y_test}))
            #print('MSE Train: ', mse_train[-1])
            #print('MSE Test: ', mse_test[-1])
from keras.wrappers.scikit_learn import KerasClassifier
keras_model = build_model()
keras_model.fit(X_train, y_train, batch_size=256, epochs=10, verbose=1)

from sklearn.metrics import roc_curve
y_pred_keras = keras_model.predict(X_test).ravel()
fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_keras)

from sklearn.metrics import auc
auc_keras = auc(fpr_keras, tpr_keras)

from sklearn.ensemble import RandomForestClassifier
# Supervised transformation based on random forests
rf = RandomForestClassifier(max_depth=3, n_estimators=10)
rf.fit(X_train, y_train)

y_pred_rf = rf.predict_proba(X_test)[:, 1]
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_rf)
auc_rf = auc(fpr_rf, tpr_rf) 

#plot roc
plt.figure(1)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))
plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve')
plt.legend(loc='best')
plt.show()
# Zoom in view of the upper left corner.
plt.figure(2)
plt.xlim(0, 0.2)
plt.ylim(0.8, 1)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))
plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve (zoomed in at top left)')
plt.legend(loc='best')
plt.show()


writer = tf.summary.FileWriter("./tfTest")
writer.add_graph(net.graph)
net.close()
print('run complete.')
